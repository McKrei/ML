{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductEncoder:\n",
    "    def __init__(self, id_list):\n",
    "        self.product_idx = {}\n",
    "        self.product_pid = {}\n",
    "        for idx, pid in enumerate(id_list):\n",
    "            self.product_idx[pid] = idx\n",
    "            self.product_pid[idx] = pid\n",
    "\n",
    "    def toIdx(self, x):\n",
    "        if type(x) == int:\n",
    "            pid = x\n",
    "            return self.product_idx[pid]\n",
    "        return [self.product_idx[pid] for pid in x]\n",
    "\n",
    "    def toPid(self, x):\n",
    "        if type(x) == int:\n",
    "            idx = x\n",
    "            return self.product_pid[idx]\n",
    "        return [self.product_pid[idx] for idx in x]\n",
    "\n",
    "    @property\n",
    "    def num_products(self):\n",
    "        return len(self.product_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = pd.read_csv('data/hist_data.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "full_df = pd.concat([hist_data.iloc[:, :-1], test])\n",
    "full_df = full_df.assign(sum_price = full_df['count'] * full_df.price_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = list(full_df.item_id.unique())\n",
    "buyer_list = list(full_df.buyer_id.unique())\n",
    "product_encoder = ProductEncoder(items_list)\n",
    "user_encoder = ProductEncoder(buyer_list)\n",
    "n_users = len(buyer_list)\n",
    "n_items = len(items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df.groupby(['buyer_id', 'item_id']).agg({'count': np.size})\n",
    "data = [(user_encoder.toIdx(ui[0]), product_encoder.toIdx(ui[1]), c) for ui, c in zip(df.index, df['count'].to_list())]\n",
    "new_df = pd.DataFrame(data, columns=['buyer_id', 'item_id', 'count']).sort_values('buyer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness\n",
    "# ['buyer_id', 'item_id', 'count']\n",
    "\n",
    "# Note: This isn't 'good' practice, in a MLops sense but we'll roll with this since the data is already loaded in memory.\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = new_df.copy()\n",
    "        \n",
    "        # Extract all user IDs and movie IDs\n",
    "        users = buyer_list.copy()\n",
    "        items = items_list.copy()\n",
    "        \n",
    "        #--- Producing new continuous IDs for users and movies ---\n",
    "\n",
    "        # Unique values : index\n",
    "        self.userid2idx = user_encoder.product_idx.copy()\n",
    "        self.itemid2idx = product_encoder.product_idx.copy()\n",
    "        \n",
    "        # Obtained continuous ID for users and movies\n",
    "        self.idx2userid = user_encoder.product_pid.copy()\n",
    "        self.idx2itemid = product_encoder.product_pid.copy()\n",
    "        \n",
    "        # return the id from the indexed values as noted in the lambda function down below.\n",
    "        # self.ratings.item_id = new_df.item_id.apply(lambda x: self.itemid2idx[x])\n",
    "        # self.ratings.buyer_id = new_df.buyer_id.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['count'], axis=1).values\n",
    "        self.y = self.ratings['count'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(8):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "         if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[0.0400, 0.0409, 0.0167,  ..., 0.0388, 0.0229, 0.0383],\n",
      "        [0.0365, 0.0457, 0.0500,  ..., 0.0330, 0.0010, 0.0237],\n",
      "        [0.0164, 0.0459, 0.0431,  ..., 0.0388, 0.0076, 0.0367],\n",
      "        ...,\n",
      "        [0.0249, 0.0337, 0.0041,  ..., 0.0378, 0.0453, 0.0441],\n",
      "        [0.0325, 0.0129, 0.0011,  ..., 0.0379, 0.0079, 0.0238],\n",
      "        [0.0282, 0.0309, 0.0130,  ..., 0.0163, 0.0440, 0.0022]])\n",
      "item_factors.weight tensor([[1.3456e-02, 4.7295e-02, 4.8008e-02,  ..., 1.8273e-02, 1.7509e-02,\n",
      "         1.9464e-02],\n",
      "        [1.2806e-03, 5.0721e-03, 4.6130e-02,  ..., 3.2279e-02, 3.1882e-02,\n",
      "         4.9365e-02],\n",
      "        [2.8263e-02, 2.7758e-02, 9.4749e-03,  ..., 8.6548e-03, 2.5004e-02,\n",
      "         1.2371e-02],\n",
      "        ...,\n",
      "        [6.7294e-05, 1.7693e-02, 9.9693e-03,  ..., 2.0468e-02, 4.4567e-02,\n",
      "         1.3423e-02],\n",
      "        [1.9633e-02, 3.5446e-02, 2.4741e-02,  ..., 3.6165e-04, 4.5476e-02,\n",
      "         8.6686e-03],\n",
      "        [9.7256e-03, 9.0458e-03, 4.8052e-02,  ..., 1.5226e-02, 6.7338e-03,\n",
      "         9.5250e-03]])\n"
     ]
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for movies and users.\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "          uw = param.data\n",
    "          c +=1\n",
    "        else:\n",
    "          iw = param.data\n",
    "        #print('param_data', param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54596"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()\n",
    "\n",
    "len(trained_movie_embeddings) # unique movie factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''It can be seen here that the movies that are in the same cluster tend to have\n",
    "similar genres. Also note that the algorithm is unfamiliar with the movie name\n",
    "and only obtained the relationships by looking at the numbers representing how\n",
    "users have responded to the movie selections.'''\n",
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  item_ = []\n",
    "  for itidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    item_id = product_encoder.toPid(int(itidx))\n",
    "    rat_count = new_df.loc[new_df['item_id'] == item_id].count()[0]\n",
    "    item_.append((item_id, rat_count))\n",
    "  for mov in sorted(item_, key=lambda tup: tup[1], reverse=True)[:20]:\n",
    "    print(\"\\t\", mov[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03130e1e88e13a032f69981c32da9b604f050f5857d0cf25fc1ed9951001c58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
